{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspired by https://github.com/karpathy/makemore\n",
    "\n",
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words[:1] = ['emma']\n",
      "len(words) = 32033\n"
     ]
    }
   ],
   "source": [
    "print(f'words[:1] = {words[:1]}')\n",
    "print(f'len(words) = {len(words)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set length = 6407, dev_set length = 801, test_set length = 801, total = 32033\n"
     ]
    }
   ],
   "source": [
    "# split dataset\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "indices = np.arange(len(words))\n",
    "indices = np.random.permutation(indices)\n",
    "\n",
    "train_indices = indices[:int(0.8*len(words))]\n",
    "dev_indices = indices[int(0.8*len(words)):int(0.9*len(words))]\n",
    "test_indices = indices[int(0.9*len(words)):int(len(words))]\n",
    "\n",
    "train_set = Subset(words, train_indices)\n",
    "dev_set = Subset(words, dev_indices)\n",
    "test_set = Subset(words, test_indices)\n",
    "\n",
    "train_dl = DataLoader(train_set, batch_size=4, shuffle=True, num_workers=2)\n",
    "dev_dl = DataLoader(dev_set, shuffle=True ,batch_size=4, num_workers=2)\n",
    "test_dl = DataLoader(test_set, shuffle=True,batch_size=4, num_workers=2)\n",
    "\n",
    "print(f'train_set length = {len(train_dl)}, dev_set length = {len(dev_dl)}, test_set length = {len(test_dl)}, total = {len(train_set) + len(dev_set) + len(test_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch = 0, Value = ['makylah', 'jennavieve', 'adaiah', 'andersen']\n",
      "Batch = 100, Value = ['rayley', 'july', 'jahmari', 'carma']\n",
      "Batch = 200, Value = ['sarim', 'zyrie', 'saunders', 'amiaya']\n",
      "Batch = 300, Value = ['noria', 'rheign', 'samarth', 'eliannie']\n",
      "Batch = 400, Value = ['ajani', 'lojain', 'chiziterem', 'hilario']\n",
      "Batch = 500, Value = ['janai', 'jakiah', 'moshe', 'journie']\n",
      "Batch = 600, Value = ['shmiel', 'mana', 'brenleigh', 'evalina']\n",
      "Batch = 700, Value = ['samyar', 'macklyn', 'corrin', 'zaiah']\n",
      "Batch = 800, Value = ['ryon', 'jaileen', 'nyliah', 'lucis']\n",
      "Batch = 900, Value = ['valaya', 'harlee', 'gowri', 'melly']\n",
      "Batch = 1000, Value = ['brighten', 'kandace', 'hyland', 'jaimie']\n",
      "Batch = 1100, Value = ['feyza', 'stormy', 'dreyah', 'flynn']\n",
      "Batch = 1200, Value = ['raelyn', 'leyna', 'annabella', 'cody']\n",
      "Batch = 1300, Value = ['ramos', 'cashius', 'brynne', 'kalder']\n",
      "Batch = 1400, Value = ['aryani', 'kalana', 'mylez', 'mosley']\n",
      "Batch = 1500, Value = ['ayan', 'thea', 'irelyn', 'gerald']\n",
      "Batch = 1600, Value = ['dedan', 'aamaya', 'nakayah', 'kiko']\n",
      "Batch = 1700, Value = ['rafaella', 'rin', 'nahzir', 'tayshawn']\n",
      "Batch = 1800, Value = ['zyion', 'jyasia', 'zamyiah', 'micky']\n",
      "Batch = 1900, Value = ['dacion', 'charlese', 'desi', 'doaa']\n",
      "Batch = 2000, Value = ['zaul', 'joceline', 'emory', 'ariyanna']\n",
      "Batch = 2100, Value = ['bryan', 'faryn', 'anson', 'destin']\n",
      "Batch = 2200, Value = ['zahli', 'kamdyn', 'saadiq', 'kairie']\n",
      "Batch = 2300, Value = ['eliana', 'timberlee', 'daylen', 'yaretzi']\n",
      "Batch = 2400, Value = ['analyssa', 'raeden', 'covan', 'iveth']\n",
      "Batch = 2500, Value = ['kailie', 'alithea', 'klaryssa', 'ellis']\n",
      "Batch = 2600, Value = ['zander', 'deniz', 'siren', 'kelayah']\n",
      "Batch = 2700, Value = ['ricardo', 'mikelle', 'leonidas', 'ryelyn']\n",
      "Batch = 2800, Value = ['fabian', 'colton', 'sanjeev', 'camber']\n",
      "Batch = 2900, Value = ['saba', 'psalm', 'viyona', 'mckinnon']\n",
      "Batch = 3000, Value = ['aris', 'rhian', 'nianna', 'samatar']\n",
      "Batch = 3100, Value = ['jefe', 'aneesh', 'pau', 'kyana']\n",
      "Batch = 3200, Value = ['nassim', 'kobain', 'mate', 'theoden']\n",
      "Batch = 3300, Value = ['effie', 'jeryl', 'yelina', 'antuan']\n",
      "Batch = 3400, Value = ['maily', 'niaomi', 'abrahm', 'daysha']\n",
      "Batch = 3500, Value = ['gracielynn', 'brecker', 'madicyn', 'jaquarius']\n",
      "Batch = 3600, Value = ['nakshatra', 'mihika', 'bismah', 'noely']\n",
      "Batch = 3700, Value = ['arun', 'farhiya', 'mizael', 'ryley']\n",
      "Batch = 3800, Value = ['ezreal', 'hudson', 'zayven', 'ginger']\n",
      "Batch = 3900, Value = ['alessandra', 'apollos', 'valarie', 'jaye']\n",
      "Batch = 4000, Value = ['matviy', 'tuba', 'jermell', 'ellaina']\n",
      "Batch = 4100, Value = ['lilyan', 'timo', 'vaanya', 'hasaan']\n",
      "Batch = 4200, Value = ['nithya', 'devonna', 'eno', 'zaedyn']\n",
      "Batch = 4300, Value = ['amouri', 'jebediah', 'tritt', 'coyote']\n",
      "Batch = 4400, Value = ['mai', 'breckyn', 'maasai', 'hinda']\n",
      "Batch = 4500, Value = ['arlin', 'yuktha', 'audy', 'aro']\n",
      "Batch = 4600, Value = ['roniel', 'yaremi', 'aryssa', 'abie']\n",
      "Batch = 4700, Value = ['adalys', 'heloisa', 'zora', 'kaesyn']\n",
      "Batch = 4800, Value = ['charley', 'audrie', 'kensy', 'solo']\n",
      "Batch = 4900, Value = ['izzy', 'keerthana', 'yaser', 'baker']\n",
      "Batch = 5000, Value = ['marlow', 'arion', 'ariday', 'apolline']\n",
      "Batch = 5100, Value = ['abdulqadir', 'abigayle', 'blair', 'calvary']\n",
      "Batch = 5200, Value = ['elham', 'strider', 'davarian', 'macarius']\n",
      "Batch = 5300, Value = ['barbie', 'ronit', 'meshal', 'evia']\n",
      "Batch = 5400, Value = ['ned', 'aleece', 'jadrian', 'aziya']\n",
      "Batch = 5500, Value = ['zahi', 'cort', 'eastin', 'kinley']\n",
      "Batch = 5600, Value = ['biruk', 'yumiko', 'bhavya', 'kimya']\n",
      "Batch = 5700, Value = ['bettie', 'stefania', 'perfect', 'ario']\n",
      "Batch = 5800, Value = ['dreamer', 'maysoon', 'trevaughn', 'noahh']\n",
      "Batch = 5900, Value = ['chaz', 'dionicio', 'isara', 'dayzee']\n",
      "Batch = 6000, Value = ['eviee', 'eniya', 'arrianna', 'nuhamin']\n",
      "Batch = 6100, Value = ['taji', 'iva', 'brynn', 'moustapha']\n",
      "Batch = 6200, Value = ['neill', 'briar', 'willard', 'leily']\n",
      "Batch = 6300, Value = ['vail', 'rook', 'berish', 'aleyshka']\n",
      "Batch = 6400, Value = ['ashling', 'maycen', 'tziry', 'toni']\n"
     ]
    }
   ],
   "source": [
    "for batch, V in enumerate(train_dl):\n",
    "  if batch % 100 == 0:\n",
    "    print(f'Batch = {batch}, Value = {V}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z',\n",
       " 0: '.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "N = torch.zeros((27, 27, 27), dtype=torch.int32)\n",
    "\n",
    "chars = sorted(set(list(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "itos\n",
    "# stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    ix3 = stoi[ch3]\n",
    "    # print(ix1, ix2, ix3)\n",
    "    N[ix1, ix2, ix3] += 1\n",
    "\n",
    "# N[0, 5, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 27, 27])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = (N+1).float()\n",
    "P /= P.sum(2, keepdims=True)\n",
    "P[0, 0].sum()\n",
    "P.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "p = P[0, 0]\n",
    "torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cexzm.\n",
      "zeale.\n",
      "riror.\n",
      "kayha.\n",
      "vinimitta.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "for i in range(5):\n",
    "    out =[]\n",
    "    ix1 = 0\n",
    "    ix2 = 0\n",
    "    while True:\n",
    "        p = P[ix1, ix2]\n",
    "        ix1 += 1 # for no obvious reason\n",
    "        ix2 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix2])\n",
    "        if ix2==0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likely_hood=tensor(-410414.9688)\n",
      "nnl=tensor(410414.9688)\n",
      "2.092747449874878\n"
     ]
    }
   ],
   "source": [
    "log_likely_hood = 0.0\n",
    "n = 0\n",
    "\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    ix3 = stoi[ch3]\n",
    "    prob = P[ix1, ix2, ix3]\n",
    "    logprob = torch.log(prob)\n",
    "    log_likely_hood += logprob\n",
    "    n+=1\n",
    "\n",
    "print(f'{log_likely_hood=}')\n",
    "nnl = -log_likely_hood\n",
    "print(f'{nnl=}')\n",
    "print(f'{nnl/n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n"
     ]
    }
   ],
   "source": [
    "xs, ys = [], []\n",
    "\n",
    "for w in words[:1]:\n",
    "  print(w)\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    ix3 = stoi[ch3]\n",
    "    xs.append(ix1)\n",
    "    xs.append(ix2)\n",
    "    ys.append(ix3)\n",
    "    \n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "ls = []\n",
    "xs, ys = [], []\n",
    "\n",
    "for _, wd in enumerate(train_dl):\n",
    "  xl = random.choice(wd)\n",
    "  ls.append(xl)\n",
    "\n",
    "for w in ls[:1]:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    ix3 = stoi[ch3]\n",
    "    xs.append(ix1)\n",
    "    xs.append(ix2)\n",
    "    ys.append(ix3)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "\n",
    "# print(list(ls[:1][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "yenc = F.one_hot(ys, num_classes=27).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16963a2e0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.imshow(xenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 27])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn((27, 27))\n",
    "xenc @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(xenc @ W)[3, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)\n",
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "logits = xenc @ W\n",
    "counts = logits.exp()\n",
    "probs = counts/counts.sum(1, keepdim=True)\n",
    "loss = -probs[torch.arange(ys.shape[0]), ys].log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.028284549713135"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.grad = None\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.data = -100 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7505834102630615\n",
      "3.664081335067749\n",
      "3.5785014629364014\n",
      "3.4938840866088867\n",
      "3.410271167755127\n",
      "3.327707290649414\n",
      "3.24623703956604\n",
      "3.165907144546509\n",
      "3.086764097213745\n",
      "3.008855104446411\n",
      "2.9322261810302734\n",
      "2.8569226264953613\n",
      "2.7829887866973877\n",
      "2.7104668617248535\n",
      "2.6393964290618896\n",
      "2.5698142051696777\n",
      "2.5017552375793457\n",
      "2.435248851776123\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(100):\n",
    "  \n",
    "  if loss.item() < 2.45:\n",
    "    break\n",
    "  \n",
    "  # forward pass\n",
    "  xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "  logits = W[xs]\n",
    "  # logits = xenc @ W # predict log-counts\n",
    "  counts = logits.exp() # counts, equivalent to N\n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "  loss = -probs[torch.arange(ys.shape[0]), ys].log().mean() + 0.01*(W**2).mean()\n",
    "  print(loss.item())\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None # set to zero the gradient\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  W.data += -0.5 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aoxzm.\n",
      "aoglkuryicqzktyhmmvmzimj.\n",
      "aoinnikfukzkktdaosfcxvpubjtbhrygotzx.\n",
      "eizixqctvuj.\n",
      "aotedogkkj.\n"
     ]
    }
   ],
   "source": [
    "# finally, sample from the 'neural net' model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "  \n",
    "  out = []\n",
    "  ix1 = 0\n",
    "  ix2 = 0\n",
    "  while True:\n",
    "  \n",
    "    ix1 = ix2 # for no obvious reason, but it gives better results than `ix2 = ix2`\n",
    "    xenc = F.one_hot(torch.tensor([ix1]), num_classes=27).float()\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    # ----------\n",
    "    \n",
    "    ix2 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(itos[ix2])\n",
    "    if ix2 == 0:\n",
    "      break\n",
    "  print(''.join(out))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "trigram example 1: .l (indexes 0,12)\n",
      "input to the neural net: 0 1\n",
      "output probabilities from the neural net: tensor([1.2379e-04, 8.8224e-01, 2.4673e-04, 2.7697e-04, 2.3130e-04, 1.1268e-01,\n",
      "        2.8302e-04, 2.1133e-04, 2.4191e-04, 1.8828e-04, 2.6277e-04, 1.9784e-04,\n",
      "        2.5826e-04, 2.6161e-04, 1.4424e-04, 9.8902e-06, 1.2438e-04, 2.8385e-04,\n",
      "        2.0616e-04, 2.7207e-04, 1.8139e-04, 2.5172e-04, 2.8226e-04, 2.2170e-04,\n",
      "        2.4858e-04, 3.2842e-05, 3.6701e-05], grad_fn=<SelectBackward0>)\n",
      "label (actual next character): 12\n",
      "probability assigned by the net to the the correct character: 0.00025826029013842344\n",
      "log likelihood: -8.261542320251465\n",
      "negative log likelihood: 8.261542320251465\n",
      "--------\n",
      "trigram example 2: ei (indexes 5,9)\n",
      "input to the neural net: 5 6\n",
      "output probabilities from the neural net: tensor([0.0097, 0.0097, 0.0097, 0.0097, 0.0097, 0.0097, 0.0097, 0.0097, 0.0097,\n",
      "        0.0097, 0.3790, 0.0097, 0.0097, 0.0097, 0.3790, 0.0097, 0.0097, 0.0097,\n",
      "        0.0097, 0.0097, 0.0097, 0.0097, 0.0097, 0.0097, 0.0097, 0.0097, 0.0097],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "label (actual next character): 9\n",
      "probability assigned by the net to the the correct character: 0.009679500944912434\n",
      "log likelihood: -4.637744903564453\n",
      "negative log likelihood: 4.637744903564453\n",
      "--------\n",
      "trigram example 3: ee (indexes 5,5)\n",
      "input to the neural net: 5 6\n",
      "output probabilities from the neural net: tensor([0.0097, 0.0097, 0.0097, 0.0097, 0.0097, 0.0097, 0.0097, 0.0097, 0.0097,\n",
      "        0.0097, 0.3790, 0.0097, 0.0097, 0.0097, 0.3790, 0.0097, 0.0097, 0.0097,\n",
      "        0.0097, 0.0097, 0.0097, 0.0097, 0.0097, 0.0097, 0.0097, 0.0097, 0.0097],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "label (actual next character): 5\n",
      "probability assigned by the net to the the correct character: 0.009679500944912434\n",
      "log likelihood: -4.637744903564453\n",
      "negative log likelihood: 4.637744903564453\n",
      "--------\n",
      "trigram example 4: lt (indexes 12,20)\n",
      "input to the neural net: 12 13\n",
      "output probabilities from the neural net: tensor([0.0097, 0.0097, 0.0097, 0.0097, 0.0097, 0.0097, 0.0097, 0.0097, 0.0097,\n",
      "        0.3790, 0.0097, 0.0097, 0.0097, 0.3790, 0.0097, 0.0097, 0.0097, 0.0097,\n",
      "        0.0097, 0.0097, 0.0097, 0.0097, 0.0097, 0.0097, 0.0097, 0.0097, 0.0097],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "label (actual next character): 20\n",
      "probability assigned by the net to the the correct character: 0.009679500944912434\n",
      "log likelihood: -4.637744903564453\n",
      "negative log likelihood: 4.637744903564453\n",
      "=========\n",
      "average negative log likelihood, i.e. loss = 4.434955596923828\n"
     ]
    }
   ],
   "source": [
    "# testing the loss on the testing set\n",
    "\n",
    "import random\n",
    "\n",
    "ls = []\n",
    "xs, ys = [], []\n",
    "\n",
    "for _, wd in enumerate(dev_dl):\n",
    "  xl = random.choice(wd)\n",
    "  ls.append(xl)\n",
    "\n",
    "for w in ls[:1]:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    ix3 = stoi[ch3]\n",
    "    xs.append(ix1)\n",
    "    xs.append(ix2)\n",
    "    ys.append(ix3)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "\n",
    "nlls = torch.zeros(5)\n",
    "for i in range(4):\n",
    "  # i-th bigram:\n",
    "  x = xs[i].item() # input character index\n",
    "  y = ys[i].item() # label character index\n",
    "  print('--------')\n",
    "  print(f'trigram example {i+1}: {itos[x]}{itos[y]} (indexes {x},{y})')\n",
    "  print('input to the neural net:', x, x+1)\n",
    "  print('output probabilities from the neural net:', probs[i])\n",
    "  print('label (actual next character):', y)\n",
    "  p = probs[i, y]\n",
    "  print('probability assigned by the net to the the correct character:', p.item())\n",
    "  logp = torch.log(p)\n",
    "  print('log likelihood:', logp.item())\n",
    "  nll = -logp\n",
    "  print('negative log likelihood:', nll.item())\n",
    "  nlls[i] = nll\n",
    "\n",
    "print('=========')\n",
    "print('average negative log likelihood, i.e. loss =', nlls.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
