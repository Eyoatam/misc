{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import tarfile\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data():\n",
    "    url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "    file_path = os.path.join(\"data\", \"cifar-10-python.tar.gz\")\n",
    "\n",
    "    # Download the dataset\n",
    "    if not os.path.isfile(file_path):\n",
    "        response = requests.get(url, stream=True)\n",
    "        if not os.path.isdir(\"data/\"):\n",
    "            os.makedirs(\"data/\")\n",
    "        with open(file_path, \"wb\") as file:\n",
    "            # receive 8kb chunks\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    file.write(chunk)\n",
    "    else:\n",
    "        # Extract the dataset and remove the tar file after extraction\n",
    "        with tarfile.open(file_path, \"r:gz\") as tar:\n",
    "            tar.extractall(\"data/CIFAR-10\")\n",
    "            os.remove(file_path)\n",
    "\n",
    "\n",
    "def load_batch(file_path):\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        batch = pickle.load(file, encoding=\"bytes\")\n",
    "        return batch\n",
    "\n",
    "\n",
    "def load_CIFAR10(folder_path):\n",
    "    train_batches = []\n",
    "    test_batch = None\n",
    "\n",
    "    if not os.path.isdir(folder_path):\n",
    "        download_data()\n",
    "        print(\"Downloaded CIFAR-10 dataset to: data/cifar-10-batches-py\")\n",
    "    \n",
    "    for file_name in os.listdir(folder_path):\n",
    "        # print(file_name)\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if \"data_batch\" in file_name:\n",
    "            train_batches.append(load_batch(file_path))\n",
    "        elif \"test_batch\" in file_name:\n",
    "            test_batch = load_batch(file_path)\n",
    "\n",
    "    # print(train_batches)\n",
    "    train_data = np.concatenate([batch[b\"data\"] for batch in train_batches])\n",
    "    train_labels = np.concatenate([batch[b\"labels\"] for batch in train_batches])\n",
    "\n",
    "    test_data = test_batch[b\"data\"]\n",
    "    test_labels = np.array(test_batch[b\"labels\"])\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 3072)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_folder = \"data/CIFAR-10/cifar-10-batches-py\"\n",
    "Xtr, Ytr, Xte, Yte = load_CIFAR10(dataset_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNearestNeighbor(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        # the nearest neighbor classifier simply remembers all the training data\n",
    "        self.Xtr = X\n",
    "        self.ytr = Y\n",
    "\n",
    "    def predict(self, X, k=1): \n",
    "        num_images = X.shape[0]\n",
    "        Ypred = np.zeros(num_images, dtype=self.ytr.dtype)\n",
    "\n",
    "        for i in range(num_images):\n",
    "            # find the k-nearest training images to the i'th image\n",
    "            # using the L1 distance (sum of absolute value differences)\n",
    "            distances = np.sum(np.abs(self.Xtr - X[i, :]), axis=1) # broadcasting happens and X[i, :] is broadcasted to match (50000, 3072)\n",
    "            # get the indices of the k smallest distances\n",
    "            min_indices = np.argsort(distances)[:k]\n",
    "            # get the labels of the k nearest examples\n",
    "            k_nearest_labels = self.ytr[min_indices]\n",
    "            # predict the label based on the majority class among k-nearest neighbors\n",
    "            unique_labels, counts = np.unique(k_nearest_labels, return_counts=True)\n",
    "            predicted_label = unique_labels[np.argmax(counts)]\n",
    "\n",
    "            Ypred[i] = predicted_label\n",
    "\n",
    "        return Ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.249200\n"
     ]
    }
   ],
   "source": [
    "nn = KNearestNeighbor()\n",
    "nn.train(Xtr, Ytr)\n",
    "# takes roughly 25 minutes to make the predictions. \n",
    "# calculating L1 distance for every 10000 of the testing images is computationally expensive \n",
    "# 10000 * whatever time it takes to calculate the distance for a single image\n",
    "prediction = nn.predict(Xte)\n",
    "print(\"accuracy: %f\" % (np.mean(prediction == Yte))) \n",
    "# untrained model = 10% accuracy (10 classes, 1/10). KNN = 25% accuracy, which is really bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
